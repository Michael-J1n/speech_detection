## 2025 NOAI -- 合成语音检测器

### 一. 题目描述

本题目是一个使用 PyTorch 实现的 ResNet 架构的深度学习项目，用于从人类录音中检测合成语音。

在现实生活中，合成语音（即 AI 生成的语音）已被广泛使用。虽然这项技术已经取得了显著进展，但它也引发了人们对潜在滥用的担忧，例如深度伪造和虚假信息。区分合成语音和真实人类语音的能力对于各种应用至关重要，包括内容验证，安全，和 AI 生成媒体中的道德考虑。生成模型的快速发展使得区分合成和真实人类录音越来越困难。本项目旨在开发一个能够有效区分这两种音频样本的模型。

> 注：本项目仅用于教育目的。使用的数据集是公开可用的，并且规模有限，使其适合在本竞赛中进行快速训练。该数据集收集于 2019 年，与最新模型相比，检测该数据集中的合成语音要容易得多。实际应用中的合成语音检测更具挑战性，需要更复杂的解决方案。该模型的表现可能无法很好地推广到现实场景中。

### 二. 数据集

本题的使用的原始数据是人类语音与合成语音的原始音频文件。然而，由于音频文件无法直接用于训练，因此需要先将音频转化为梅尔频谱图 (Mel Spectrum)。其生成过程可简单概括为如下步骤：

1. 首先对音频进行分帧、加窗处理；
2. 然后进行短时傅里叶变换（STFT），得到时频域信息；
3. 接着用一组梅尔滤波器将线性频谱投影到梅尔尺度上（模拟人耳对频率的感知）；
4. 最后取对数，得到最终的梅尔频谱图。

具体来说，梅尔频率 $m$ 与真实频率 $f$（单位 Hz）之间的转换关系为：

$$
m = 2595 \cdot \lg\left(1 + \frac{f}{700}\right).
$$

例如，假设我们有一个真实人类语音样本，时长约为 3 秒，采样率为 16kHz。我们将其转换为梅尔频谱图，使用如下常规参数：

- 采样率：16000 Hz
- 梅尔滤波器数量：128
- 帧移（hop length）：512
- FFT 窗口长度：1024

那么该音频在转换为梅尔频谱图后，将得到一个形如：

```
torch.Size([1, 128, 94])
```

的张量，其中：

- `1` 表示音频为单通道；
- `128` 表示频率轴上共 128 个梅尔频带；
- `94` 表示在时间轴上总共分为了 94 帧。

在视觉上，它是一张二维图像，其中横轴为时间帧，纵轴为梅尔频率。

然而，由于这样的处理较为繁琐，因此本题目提供的数据集为原始音频文件转化过后的梅尔频谱图。其对应的频谱图均会保存为张量，以 `.pt` 格式进行存储。在题目的文件目录下，`dataset` 文件夹存储了所有训练数据。

首先，`dataset` 的地址为 [dataset]()，主要含有以下三个文件夹：首先，`training_set` 文件夹中存储了所有训练数据并将用于模型训练，`validation_set` 文件夹中存储了所有验证数据并将以此数据集的预测结果作为 A 榜成绩，`testing_set` 文件夹中存储了所有测试数据并将以此数据集的预测结果作为 B 榜成绩。带有`bonafide` 的文件名为真实人类录音的频谱图，而 `spoof` 文件夹中存储了所有合成语音的频谱图。

除去上述文件夹外，`dataset`中还含有脚本 `spectrogram_dataset.py`。 这是一个用于帮助加载频谱图并为训练模型提供 `PyTorch` 中 `Dataset` 接口的脚本。它将遍历每个数据集的子目录，并帮助标注标签（`bonafide` 为 0，`spoof` 为 1）。其 `__getitem__` 魔法方法将会返回一个形如 `{ 'spectrogram': Tensor, 'label': Tensor, 'path': str }` 的字典，其中`spectrogram` 为频谱图，`label` 为标签，`path` 为该频谱图的路径。在实际使用中，需要在程序中由以下语句导入：`from data.spectrogram_dataset import SpectrogramDataset`，然后即可以正常使用，例如`train_dataset = SpectrogramDataset('data/training_set')`。具体使用方法也可以参照 `baseline.py`。

> 注意，在编写程序时请勿修改上述脚本，否则可能导致数据集加载错误。

### 三. 任务

本项目旨在建立一个能够区分合成（AI 生成）语音和真实人类录音的模型。它使用从音频样本生成的频谱图作为 ResNet-based 神经网络的输入。你可以通过 `from torchvision.models import resnet18, ResNet18_Weights` 来加载在 ImageNet 数据集上预训练的 ResNet18 模型以及其参数。具体使用方法可见 `baseline.py` 。你的具体任务即在此基础上修改模型结构并在训练集上进行训练，以达到对于合成语音的良好检测效果。

### 四. 提交

选手需提交一个名为 submission.zip 的压缩包，压缩包中应包含以下两个文件：
submissionA.csv：包含模型在验证集（validation_set）上的预测标签结果，每行一个 0 或 1，不需要文件名；
submissionB.csv：包含模型在测试集（testing_set）上的预测标签结果，每行一个 0 或 1，不需要文件名。
系统将读取 submission.zip，并根据预测结果与真实标签计算 Public 分数（A 榜）和 Private 分数（B 榜），作为选手成绩排名依据。提交文件必须严格按照上述格式，否则系统将无法正确读取。具体提交流程的代码也可参照`baseline.py`：[baseline.py]()。

### 五. 评分

评分规则为将选手提交的 csv 文件与 `ground_truth_labels` 进行比对，以确定网络结构的预测能力。具体评分标准使用 F1-score，用于衡量模型在二分类任务中的整体性能。其定义如下：
$$ \text{F1-score} = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}$$其中，Precision表示模型预测为正类的样本中实际为正类的比例，Recall表示实际为正类的样本中被模型正确预测为正类的比例。最终成绩将在 $0$ 至 $1$ 之间，越靠近 $1$ 表示模型性能越好。
